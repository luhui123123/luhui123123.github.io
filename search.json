[{"title":"Hello World","url":"/2022/08/30/hello-world/","content":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub.\nQuick StartCreate a new post$ hexo new &quot;My New Post&quot;\n\nMore info: Writing\nRun server$ hexo server\n\nMore info: Server\nGenerate static files$ hexo generate\n\nMore info: Generating\nDeploy to remote sites$ hexo deploy\n\nMore info: Deployment\n","categories":["hello"]},{"title":"CI/CD最佳实践之路","url":"/2021/12/30/devops/CICD%E6%9C%80%E4%BD%B3%E5%AE%9E%E8%B7%B5%E4%B9%8B%E8%B7%AF/","content":"\n本文旨在介绍ZBJ DevOps团队倾力打造的DevOps平台中关于CI/CD流水线部分的实践。历经三次大版本迭代更新的流水线，完美切合ZBJ各种业务发展需求，在满足高频率交付的同时，提高了研发效率，降低了研发成本，保证了交付质量。\n持续集成（Continuous Integration）简称CI，持续集成强调开发人员提交了新代码之后，立刻进行构建、（单元）测试。根据结果，我们可以确定新代码和原有代码能否正确地集成在一起。持续集成过程中很重视自动化测试验证结果，对可能出现的一些问题进行预警，以保障最终集成的代码没有问题。持续交付（Continuous Delivery）简称CD，持续交付在持续集成的基础上，将集成后的代码部署到更贴近真实运行环境的「类生产环境」（test，testing）中，然后交付给质量团队，以供评审。如果评审通过，代码就进入生产阶段。持续交付并不是指软件每一个改动都要尽快部署到产品环境中，它指的是任何的代码修改都可以在任何时候实施部署。有的人也把CD称为Continuous Deployment（持续部署），持续部署是指当交付的代码通过评审之后，可以部署到生产环境中。这里需要注意的是，持续部署应该是持续交付的最高阶段，持续交付是一种能力，持续部署是一种持续交付的表现方式。\n\n背景介绍在提到ZBJ DevOps流水线之前，先交代一下历史背景。2015年前，猪八戒网80%的项目都是用PHP语言开发的，剩下的少部分使用的是Nodejs和Java。2015年，ZBJ研发中心进行了自发性的“工业革命”——腾云七号行动——使用Java语言将核心业务代码进行了重构和拆解，建立了以Dubbo为核心的SOA微服务框架，使用ZooKeeper+Swoole为核心的业务调用提供机制。满足新业务使用Java语言编写、老业务仍然使用PHP编写，同时支持两种语言（Nodejs&amp;PHP）调用Dubbo服务的能力。\n\n之后，开始全面推行前后端分离，于是流行了沿用至今的主流架构：\n\nNodejs：负责前端\nJava：负责后端\nPHP：负责老项目维护\n\n剩余部分小系统或者边缘化的工具使用其他语言开发，或者在此三种语言基础上的一些变种：\n\n随着业务的重构和拆解，以前一个个庞大的系统被拆解成若干个独立的小系统，使得交付变得更加容易。\n\n而随着项目工程数量的快速增长，交付开始变得频繁，传统开发模式的一两个月交付一次远远不能满足交付要求，改变迫在眉睫。\n2016年Q3季度，感受到改变迫在眉睫，ZBJ研发中心经充分准备后决定抽调部分运维同学和开发同学组建一支名为“DevOps取经团”的团队，力图打造属于ZBJ自己的以提高研发效率为目标的平台。\nZBJ CI/CD发展史第一阶段：2015年以前2015年以前，此时“工业革命”还未开始，ZBJ所谓的流水线先后经历了“大锅饭年代”、“公交车模式”。\n\n可以看到，无论是“大锅饭年代”，还是“公交车模式”都会面临很多问题：\n\n项目耦合度太高，容易导致合并冲突，环境冲突等。\n集成过程中未对代码进行审查，错误代码发布到测试环境后，会影响依赖方的测试。\n发布受限制，必须在专门时间由专人发布。\n发布异常时，回滚工作异常艰难。\n\n第二阶段（2016-2017）：私家车模式通过“工业革命”的变革，对系统的重构和拆解，同时引入工程责任制，给每个工程指定负责人，对工程的各种权限进行了管控，业务范围和边界变得更加清晰，使得项目耦合度太高的问题得到了很大的缓解。\n解决了项目耦合度高的问题，接下来解决如何实现随时由开发团队自主发代码的问题。\n用Jira做研发流程管理，制定针对ZBJ需求上线的流程：\n\n需求上线流程每个环节对应流水线的每个环境，只有到达某个环节，才能推送对应环境，每个环节会制定对应的准入准出，保证每走到的下一步都离成功部署更近一步。\n\n每个环境制定不一样的执行任务，基本包括Jira状态校验，代码审查（单元测试），编译构建，上传包到制品库，拉取制品库中的包部署到对应环境。\n值得注意的是，我们引入了Docker发布，所以我们的流水线是支持容器和虚拟机的混合发布的。虚拟机发布方式的制品是存放在一个叫做文件服务器的地方，容器发布方式的制品是push到Harbor仓库的。\n\n采用的分支策略是：branches开发，master发布，tags存档（我们使用GitLab作为源码管理工具）。在测试环境通常为非master分支，测试完毕后合并到master，推送预发布，并针对当前版本打一个tag。针对这种情况，我们的“一次构建，处处使用”指的是测试环境用测试环境构建好的包，其他环境用预发布构建好的包。\n\n在推送每一个环境（test环境除外）时，都会校验当前版本的代码是否为前置环境推送过的最新代码，保证不会将没有经过审查的代码交付到线上。\n在测试环境每一次编译构建之前，都会对代码进行一次安全扫描，Java语言的工程通过解析pom文件对其所有的依赖进行递归扫描，Nodejs语言的工程通过对node_module里下载的包进行递归扫描，确保有安全漏洞的代码不会被带入到生产环境。\n编译构建时，会根据开发语言的不同，执行不同的编译脚本，根据发布方式的不同（虚拟机发布或者容器发布），执行不一样的后续操作步骤。\n我们将Jenkins作为后台编译服务器，采用的是多master多slave的架构，我们并未直接使用Jenkins的流水线，而是开发了一个叫Pipeline的系统，与Jenkins做对接（此时的对接方式是调用Jenkins的API），由Pipeline系统提供Jenkins作业所需要的全部信息，另外编写了整个过程需要具体执行操作的脚本，通过Jenkins的job配置“Execute shell”的方式每次在构建之前导入到工作空间。\n\n流水线标准生产过程大体如下：\n\n另外，针对回滚的情况，因为每次在上线前都会在预发布会构建一个稳定的版本，并打一个tag，并且记录下tag对应的制品（包或者镜像）版本，所以在回滚的时候，只要选择要回滚的版本，便能找到对应版本的制品，进行重新发布，以此达到回滚的目的。\n\n至此，第二阶段大体实现了以下功能：\n\n通过用Jira需求上线流程和流水线做整合，以及多种推送前的校验，保证了上线过程的每一步都是可靠的\n每一个环境的集成和发布都是自动化的\n因为过程变得可靠且自动化，使得将发布过程开放给研发团队成为了可能，达到了随时自主上线的目的。\n\n然而这样的流水线也有诸多问题：\n\n不够灵活，如在推送测试环境时，整个过程执行的步骤是固定的的，即第一步做什么，第二步做什么都是固定的，不能新增也不能删减，如某些团队需要进行单元测试，有的不需要，但流水线都会去执行单元测试，通常情况下单元测试过程是一个花费时间比较长的过程，这对于需要频繁更改和部署的业务是不友好的。\n推送成功率不高，因为整个过程是串联的，某一个环节出现错误，将会导致本次推送失败，而某些环节本不应该影响构建结果的，最后导致了构建失败。\n\n第三阶段（2017-2019）：拥有灵活车道的私家车模式考虑到前面提到的两点，ZBJ DevOps团队在17年底对流水线做了二次改造：\n\n所有执行步骤拆解成独立原子任务，建立原子任务库；\n将原子任务根据功能性分为两种，校验类以及执行类，校验类原子任务主要是是做准入准出的判断，执行类主要是编译构建，打镜像，上传hub仓库以及部署。\n将原子任务根据执行载体分为两种，Java类和Jenkins类，直接用Java程序执行的任务为Java类，需要Jenkins执行的任务为Jenkins类。\n根据开发语言、发布方式、业务类型的不同，从原子任务库中选取不同原子任务组成一条标准有序的执行流水线。\n提供工程特殊配置，如有些工程需要增强校验，有的工程需要减少校验，则可以通过启用和禁用的方式进行特殊配置，如下图所示，根据1、2和3步骤后可最终生成一条本次执行的流水线任务列表。\n\n\n\n\n根据原子任务的制定，我们将Jenkins执行的job也拆分成了对应的几类，每一类拥有足够多数量的job进行任务的执行。\n\n\n\n\n升级了Pieline系统和Jenkins通信架构，通过编写一个RabbitMQ的插件植入到Jenkins Master上，从原来调用API的方式，改成用RabbitMQ的方式进行通信，大大提高了效率和成功率。\n\n\n\n升级了Jenkins架构，构建一个能适配ZBJ所有开发语言的镜像，利用Jenkins Master的Kubernetes插件，将原来的虚拟机slave节点全部替换成容器slave节点，并且这个slave集群完全由Jenkins Master的Kubernetes插件调度，不论在高并发和低并发的时候都能及时扩缩容，满足业务需求。\n\n第四阶段（2020-至今）：智能驾驶模式可以看到，到第三个阶段为止，我们的每一次编译过程，都需要研发同学“推送一下”，而且这个过程也是需要花费一些时间的，比如一个正常的Nodejs工程平均编译时长至少需要花费100+s以上，一个正常的Java工程平均编译时长至少也是需要30s以上，由于我们提供了推送过程“可视化”的功能，且没有执行结果的通知，导致用户必须关注推送过程以确保本次推送是成功的，大大浪费了研发同学的时间。\n在此基础上，我们进一步做出了以下优化：\n\n为每一个GitLab上的工程添加一个Webhook，每当开发人员向仓库push一次，便会触发Webhook，调用Pipeline系统接口进行一次快速构建。\n\n\n注意：并不是每次push都会进行一次快速构建，为了防止开发同学频繁修改少量代码提交到版本库，我们规定了一个“暗号”，只有当开发同学在commit message中添加这个“暗号”，才会触发一次快速构建。\n\n\n\n快速构建的结果是构建一个包或者一个镜像，存放在前文提到过的文件服务器或者Harbor仓库。在下一次用户“推送”的时候，便会根据分支和版本判断是否存在已编译过的包或者镜像，如果存在，则直接使用，跳过编译过程。\n\n\n\n增加快速构建结果通知，因为整个快速构建过程是后台执行的，所以流水线系统通过企业微信的方式通知到用户本次快速构建的结果。\n\n\n\n\n除了增加快速构建的通知，我们还增加了推送的通知。用户再也不用关注推送过程，只需要在推送后继续做其他事情，推送结果由企业微信通知到用户。\n\n\n\n值得注意的是，当推送失败后，流水线系统也会通知到用户，进行对应问题的排查。\n\n至此，ZBJ的CI/CD实践之路基本介绍完毕，当然，其中也还有很多细节方面，因为涉及的东西太多，不便铺开来讲。\n总结三次重大改造的结果第一次改造：奠定了ZBJ的CI/CD基础，打造了一条标准的流水线，解放了运维劳动力（过程全自动化），提高了研发效率，降低了研发成本（运维同学由最多时候的三四十个减少到了不到十个人）。\n第二次改造：流水线实现了高可用，同时其灵活的配置能完美满足不同业务团队的需求。\n第三次改造：提升了流水线效率，弱化推送过程，增强以人为本的体验，使推送过程更加智能化。\n谈谈未来CI/CD实践之路还在继续，因为不同公司有不同的业务场景，而同一公司的业务也会随着时代的发展不断变化，只有适合自己的才是最好的，只有能拥抱变化的才是最好的，但万变不离其宗的，我觉得应该有一下几点：\n\nCI/CD应该是以提高研发效率为目标的实践，一切脱离这个目标只是为了迎合什么口号而做什么的是都是耍牛氓。而实现这个目标是一个比较漫长的过程，一开始会比较容易，后面就会越来越难，这需要不断思考和学习的过程。\nCI/CD应该是紧贴业务的，因为业务的不同，要求的技术架构也会有所不同，随之而来，要求的交付方式也会有所不同。\nCI/CD应该是以人为本的，我们应该尽可能地将一切繁琐的过程交给程序去执行，而人只需要“坐享其成”或者做少量的决策即可。\n\n","categories":["DevOps"],"tags":["CI/CD"]},{"title":"ansible-playbook目录结构","url":"/2022/01/05/devops/ansible-playbook/","content":"\nproduction                                                                   # 关于生产环境服务器的资产清单文件\ndevelop                                                                        # 关于开发环境的清单文件\ngroup_vars/                                                                  # 组变量目录\ngroup1                                                               # 组 group1 的变量文件\ngroup2                                                               # 组 group2 的变量文件\n\n\nhost_vars/                                                                     # 每个host的变量的目录\nhostname1                                                         # hostname1 定义的变量文件\nhostname2                                                         # hostname2 定义的变量文件\n\n\nlibrary/                                                                          # 如果有自定的模块，放在这里（可选）\nfilter_plugins/                                                               # 如果有自定义的过滤插件，放在这里（可选）\nsite.yml                                                                         # 指定 playbook 的统一入口文件\nwebservers.yml                                                             # 特殊任务的 playbook\ndbservers.yml                                                               # 还是特殊任务的 playbook\nroles/                                                                            # role 存放目录\ncommon/                                                           # common 角色的目录\ntasks/                                                       # common 角色的目录\nmain.yml\n\n\nhandlers/\nmain.yml\n\n\ntemplates/\nntp.conf.j2\n\n\nfiles/\nbar.txt\nfoo.sh\n\n\nvars/\nmain.yml                                         # common 角色定义的变量文件\n\n\ndefaults/\nmain.yml                                           # common 角色定义的默认变量文件（优先级低）\n\n\nmeta/\nmain.yml                                          # common 角色的依赖关系文件\n\n\n\n\nwebtier                                                                  # 下面这些都是和 common 同级的目录，是另外的一些角色\nmonitoring\nfooapp\n\n\n\n","categories":["DevOps"],"tags":["ansible"]},{"title":"ELK","url":"/2022/01/05/devops/ELK/","content":"Elasticsearch下载elasticsearch包，下载地址：https://www.elastic.co/cn/downloads/elasticsearch\n\n安装elasticsearchyum install elasticsearch-7.10.1-x86_64.rpm\n\n配置  vim /etc/elasticsearch/elasticsearch.ymlcluster.name: ixinwu#node.name es集群内唯一的namenode.name: node-1# Path to directory where to store the data (separate multiple locations by comma):#path.data: /data/es/data## Path to log files:#path.logs: /data/es/log# Set a custom port for HTTP:#http.port: 9200#集群的话需要添加这几条# The default list of hosts is [&quot;127.0.0.1&quot;, &quot;[::1]&quot;]#discovery.seed_hosts: [&quot;es1&quot;, &quot;es2&quot;,&quot;es3&quot;]## Bootstrap the cluster using an initial set of master-eligible nodes:#cluster.initial_master_nodes: [&quot;es1&quot;,&quot;es2&quot;,&quot;es3&quot;]#设置可以添加密码xpack.security.enabled: true\n\n运行命令添加密码  9nVRdtJjQMZrDYyx/usr/share/elasticsearch/bin/elasticsearch-setup-passwords interactive\nKibana下载kibana包，下载地址https://www.elastic.co/cn/downloads/kibana\n\n安装kibanayum install kibana-7.10.1-x86_64.rpm\n\n配置  vim /etc/kibana/kibana.ymlserver.host：指明服务运行的地址elasticsearch.uri： 指明elasticsearch运行的地址和端口kibana.index: 指明kibana使用的索引，这个是自定义的。tilemap.uri：指明了使用地图api， 在这里使用了高德地图的api\nnginx配置密码访问yum install httpd-toolshtpasswd -c passwd zhihuitingchecat /etc/nginx/conf.d/kibana.confserver &#123;   listen 80;   server_name kibana.ixinwu.com;   location / &#123;     proxy_pass http://172.18.58.7:5601;     proxy_set_header X-Real-IP $remote_addr;     proxy_set_header Host $host:$server_port;     proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;   &#125;&#125;\n\nFilebeat安装filebeatcurl -L -O https://artifacts.elastic.co/downloads/beats/filebeat/filebeat-7.10.1-x86_64.rpmsudo rpm -vi filebeat-7.10.1-x86_64.rpm\n配置   /etc/filebeat/filebeat.ymloutput.elasticsearch:  hosts: [&quot;&lt;es_url&gt;&quot;]  username: &quot;elastic&quot;  password: &quot;&lt;password&gt;&quot;setup.kibana:  host: &quot;&lt;kibana_url&gt;&quot;\n\nLogstash安装配置并接收redis的数据","categories":["DevOps"],"tags":["ELK"]},{"title":"gitlab","url":"/2021/12/23/devops/gitlab/","content":"这里需要安装 10.1.2 版本的 Gitlab，并把数据导入到里面，之后在这里测试 Gitlab 的升级。\n安装添加 repo\ncurl https://packages.gitlab.com/install/repositories/gitlab/gitlab-ee/script.rpm.sh | sudo bash\n\n\n安装 10.1.2 版本的 gitlab：\nsudo yum install gitlab-ee-10.1.2-ee.0.el7.x86_64\n安装结果：\n修改配置配置文件是 /etc/gitlab/gitlab.rb 。\nexternal_url 要确保为正确的ip或者域名，git的链接才正常。 之后就可以通过浏览器访问了，默认是用80端口。\n修改备份目录为 /data1/gitlab/git-backups 。\n修改数据储存目录为 /data1/gitlab/git-data 。\n全部修改的配置如下：\nexternal_url ‘http://1.2.3.4:10000&#39;gitlab_rails[‘backup_path’] = “/data1/gitlab/git-backups”gitlab_rails[‘backup_archive_permissions’] = 0644gitlab_rails[‘backup_keep_time’] = 604800git_data_dirs({ “default” =&gt; { “path” =&gt; “/data1/gitlab/git-data” } })gitlab_rails[‘gitlab_shell_ssh_port’] = 51668604800 秒是 7 天。\n然后执行命令。\n$ sudo gitlab-ctl reconfigure这个命令要执行一段时间。这个命令会把配置保存到 /var/opt/gitlab 目录中。\n目录说明/opt/gitlab 保存 Gitlab 自身的代码和依赖\n/var/opt/gitlab 保存了 gitlab-ctl reconfigure 最终写入的配置，\n/etc/gitlab 保存了可以人肉编辑的配置和证书。\n/var/log/gitlab 保存了 gitlab 的日志。\n启动第一次查看状态：\n$ sudo gitlab-ctl status发现各组件都已经启动了。。。。\n这里再启动一下保险：\n$ sudo gitlab-ctl start其他命令：\ngitlab-ctl stopgitlab-ctl restartgitlab-ctl restart sidekiqgitlab-rails console查看数据目录发现已经有数据了，备份数据目录还没有数据：\n$ sudo ls /data1/gitlab/git-data/$ sudo ls /data1/gitlab/git-backups/创建备份上面的配置已经指定了备份的目录和保存时间，下面来创建备份：\n$ sudo gitlab-rake gitlab:backup:create再次查看备份目录已经有东西了：\n$ sudo ls /data1/gitlab/git-backups/上面只是保存了 Gitlab 中的数据，即 Gitlab 中的用户、代码数据，但是没有保存 Gitlab 的配置。下面的脚本用来打包配置：\n$ sudo sh -c ‘umask 0077; tar -cf $(date “+etc-gitlab-%s.tar”) -C / etc/gitlab’亲测可行，解包验证：\n$ sudo tar xvf etc-gitlab-1586852672.tar自动备份：\n#通过crontab使用备份命令实现自动备份:0 2 * * * /opt/gitlab/bin/gitlab-rake gitlab:backup:create备份脚本 /data1/gitlab/git-backups/backup_gitlab.sh ：\n#!/bin/bash#backuping gitlab configurationsback_dir=’/data1/gitlab/git-backups/‘\ndate=date +&#39;%F-%T&#39;cd $back_dirsh -c ‘umask 0077; tar -cf $(date “+etc-gitlab-%s.tar”) -C / etc/gitlab’#backup gitlab data &amp; delete old files/bin/gitlab-rake gitlab:backup:createfind $back_dir -name “*.tar” -mtime +7 | xargs rm -f#rsync to zfs serverrsync -a –delete –password-file=/root/rsyncd.passwd $back_dir &#103;&#105;&#116;&#x6c;&#97;&#98;&#64;&#52;&#46;&#53;&#x2e;&#54;&#46;&#x37;::gitlabecho “date +%F-%T rsync done” &gt;&gt; rsync_gitlab.log这个脚本自动备份配置和数据，并且会自动删除7天前的旧备份。\nrsync命令是一个远程数据同步工具，可通过LAN/WAN快速同步多台主机间的文件,这个算法只传送两个文件的不同部分，而不是每次都整份传送，因此速度相当快。。\nrsync：\n-a 归档模式，表示以递归方式传输文件，并保持所有文件属性。–delete 删除那些目标中有，但是源地址中没有的文件。–password-file 从FILE中得到密码。另外这里使用双冒号的原因是：从本地机器拷贝文件到远程rsync服务器中！\n最后记录日志。\n编辑 /etc/crontab 设置定时任务：\n0 0 * * * root /data1/gitlab/git-backups/backup_gitlab.sh &gt; /dev/null 2&gt;&amp;1这样就实现了每日凌晨0:00 进行全量备份(数据&amp;配置文件),数据保存最近7天,配置文件保存最近7天;\n修改 root 密码执行：\n$ sudo gitlab-rails console production依次输入：\n user = User.where(id: 1).first user.password=”“ user.password_confirmation=”“ user.save! quit迁移 &amp; 恢复迁移只比恢复多了一步，就是把数据复制过来。\n复制数据时注意，别复制太大的数据，生产环境要小心！！！这里略过复制。\n先暂停服务：\n$ sudo gitlab-ctl stop unicorn$ sudo gitlab-ctl stop sidekiq再恢复数据：\n$ sudo gitlab-rake gitlab:backup:restore BACKUP=1586804022_2020_04_14_10.1.2恢复完成后重新启动：\n$ sudo gitlab-ctl start检查 GitLab 是否正常运行：\n$ gitlab-rake gitlab:check SANITIZE=true去掉注册管理员账号登录 —-&gt; 进入 Admin area (就是那个🔧) —-&gt; settings —-&gt; 取消 Sign-up enabled —&gt; save\n","categories":["DevOps"],"tags":["gitlab"]},{"title":"K8s集群升级的4种方式","url":"/2021/12/30/CloudNative/K8s%E9%9B%86%E7%BE%A4%E5%8D%87%E7%BA%A7%E7%9A%844%E7%A7%8D%E6%96%B9%E5%BC%8F/","content":"如果你已经使用 Kubernetes 一段时间了，则可能需要考虑计划定期升级。从 Kubernetes 1.19 开始，每个开源版本都提供一年的补丁。你需要升级到最新的可用次要版本或补丁版本才能获得安全性和错误修复。但是，如何在不停机的情况下升级基础架构的关键部分呢？本文将指导你了解在任何环境中升级 Kubernetes 时要考虑的常见模式。\n我们不会深入研究执行升级的所有工具和注意事项。如果你使用的是集群管理工具或托管 Kubernetes 服务，你应该查阅你的文档以获得最适合你环境的选项。你还需要注意，某些工作负载和环境可能会限制你选择的升级策略。\n我们将讨论集群升级的一些高级模式：\n\n原地升级\n蓝绿升级\n滚动升级\n金丝雀升级\n\n这些模式类似于应用程序升级选项，但由于其潜在的影响可能需要考虑一些独特的因素。升级基础设施可能会产生相当大的成本，具体取决于升级需要多长时间以及你的规模有多大。\n控制平面组件Kubernetes 控制平面由 Kubernetes API Server、etcd 数据库、controller manager,、scheduler以及你的环境中可能拥有的任何其他控制器（例如云或ingress ）组成。升级 API Server是升级集群的第一步。Kubernetes 将状态存储在 etcd 中，并且随着任何重大应用程序升级，你需要确保至少有一个备份，并且你已经验证可以恢复备份。在某些情况下，API Server升级可能还需要 etcd 升级。\n数据平面组件Kubernetes 数据平面由 kubelet、容器运行时以及你在集群工作负载中使用的任何网络、日志记录或存储驱动程序组成。对于许多集群，至少需要 kube-proxy 和 CNI 插件更新。你的数据平面组件可以小于等于你的 API Server版本。理想情况下，你的主机操作系统、容器运行时和数据平面组件可以相互独立升级。将这些组件解耦，将确保你可以在出现错误修复、新功能或安全补丁时快速升级。\nKubernetes 托管服务如果你使用Amazon Elastic Kubernetes Service (EKS)等托管 Kubernetes 服务，则会为你处理控制平面升级。如果你使用的是托管数据平面服务，例如托管节点组 (MNG)，你的数据平面升级也应该由你的云提供商自动处理。\n即使使用托管服务，你仍然有责任验证已安装在集群中的工作负载、附加控制器和第三方插件（例如 CNI）。在测试或开发环境中升级集群之前，应测试这些组件的 API 兼容性。\n在所有这些升级策略中，你应该避免在集群升级期间进行应用程序升级。如果可能，请保持你的工作负载使用相同的版本，以最大限度地减少可能错误地归因于 Kubernetes 升级的故障。还要尽量减少其他潜在问题，例如scheme升级或应用程序 API 兼容性。\n对于任何 Kubernetes 升级，你应该按以下顺序升级组件：\n\n控制平面\n数据平面和节点\n附加组件\n工作负载\n\n这些升级模式将帮助你决定如何升级最适合你的集群和环境的组件。\n01 原地升级(In-Place Upgrade)执行原地升级时，你必须格外小心，确保组件保持健康，因为你是在当前服务于生产流量的集群上执行工作。原地升级可以包括包更新（例如 yum、apt）、配置管理自动化（例如 Ansible、Chef）或 VM/容器镜像更改。理想情况下，你的升级将是脚本化和自动化的（包括回滚），但如果这是你第一次升级，在开发或测试环境中手动进行升级可能会有所帮助。\n\n原地升级意味着所有组件将大致同时升级。如果你通过配置管理更改所需的 API Server版本并推送新配置，则所有 API Server在收到新配置后都会升级。这与我们稍后讨论的滚动升级不同。\n原地升级的主要好处是：\n\n在任何规模下，它都是最快的。\n如果手动完成，则可以更好地控制组件和升级过程。\n它很容易适用于多种环境（本地或云）。\n从基础设施成本的角度来看，它是最便宜的。\n\n根据你的流程、规模和工具，原地升级可能是能够编写脚本的最直接的方法。脚本可以在本地或在开发集群中进行测试，而无需重新配置集群管理员团队可能无法访问的资源——例如负载均衡器或 DNS。\n如果要使用此方法进行升级，原地升级还需要考虑以下限制：\n\n如果你的所有 API Server或控制器同时升级，则可能导致停机。\n如果你想从 Kubernetes 1.16 迁移到 1.20，你必须将整个集群四次升级到每个次要版本。\n验证每个步骤可能是一个手动过程，这可能会增加额外的时间和出错的机会。\n你应该在失败的情况下测试回滚计划，因为某些升级无法轻松恢复。（例如，scheme更改）。\n\n02(蓝/绿升级)蓝/绿集群升级需要你使用新版本的 Kubernetes 创建第二个集群。你需要部署新的控制平面和数据平面，然后将所有工作负载复制到新集群，然后再将流量从旧集群切换到新集群。你可以使用蓝/绿来更新集群的每个组件，但整体集群升级更易于部署和回滚。\n\n好消息是，设置新集群通常比升级集群更容易。关于如何将工作负载部署到新集群，你有多种选择。如果你的工作负载已经是 GitOps 或持续交付的一部分，你可以在升级之前或期间将部署同时转到新集群和旧集群。如果你没有自动部署，你可以使用Velero 之类的工具来备份你现有的工作负载并将它们部署到新集群。\n创建新的“Green”集群可以让你对新版本按预期工作充满信心，并让你控制何时切换版本。新集群还可用于验证自动化工具，例如 Terraform 模块或 GitOps 存储库。你可以随时通过 DNS 或负载均衡器进行更改，甚至可以在维护时段或低利用率期间进行更改。\n蓝/绿升级的主要好处是：\n\n在发送流量之前预先验证所有组件是否正常。\n你可以一次升级多个版本（例如，从 1.16 直接升级到 1.20）。\n你可以更改可能难以测试的基础架构的其他部分（例如，切换区域、添加区域、更改实例类型）。\n回滚是最安全和最容易的。\n\n蓝/绿部署要考虑的缺点包括：\n\n这是基础架构成本中最昂贵的策略，因为你必须在迁移期间运行两倍的计算容量。\n如果你有数千个工作程序节点，你可能无法获得运行完整的第二个集群所需的所有计算容量。\n如果你有多个并发集群升级，则此策略很难扩展到数十个或数百个集群。\n除非你有备用服务器，否则在没有虚拟化的情况下在本地实现蓝/绿并不容易。\n如果你有很多端点要更新，一次切换所有流量可能并不容易。负载均衡器可能需要预先调整并预热缓存。请注意 DNS 生存时间 (TTL)，它可能会或可能不会用于分散负载。\n一次切换所有集群流量需要跨团队协调迁移到新集群；以及工程周期来验证工作负载的规模是否正确。\n\n当你拥有较少数量的集群或少于几百个工作节点时，蓝/绿可能是一个很好的策略。它允许你跳过版本并且回滚是安全的，但它可能会需要更多的基础设施支出和协调时间。\n03(滚动升级)如果你熟悉 Kubernetes 部署策略，你就会熟悉滚动升级。滚动升级将部署组件的一个升级为新副本，然后缩减一个旧副本。它将继续这种模式，直到所有旧组件都被删除。滚动升级的增量性质比原地升级和蓝/绿策略有一些优势。\n\n与原地升级类似，你需要一次升级 Kubernetes 的一个次要版本。当需要升级多个版本时，这可能是额外的工作，但它是唯一受支持的选项。根据你要升级的组件，你可以使用不同的工具来升级每个组件。\n对于像控制平面这样的资源，你可能希望将带有升级的 API Server的新服务器添加到控制平面，然后关闭旧服务器。如果你使用的是 AWS，则可以更改 Auto Scaling 组启动配置 AMI 并一次替换一个实例。其他控制平面组件（例如调度程序）可能在集群内作为容器运行，因此你可以使用标准的 Kubernetes 滚动部署升级来升级这些组件。\n与蓝/绿相比，滚动升级的主要区别在于你的外部流量路由（DNS 和负载均衡器）将保持指向同一位置。在进行生产集群升级之前，你需要确保在不同的集群或环境中测试所有附加组件和工作负载。\n请注意，AWS 托管节点组、kOps、Cluster-API和许多其他 Kubernetes 集群管理工具使用滚动升级策略。好处包括：\n\n与原地升级相比，更安全的更新和回滚。\n成本低于蓝色/绿色，并且资源耗尽的可能性较小。\n如果出现问题，可以在升级过程中暂停。\n可以在本地环境模拟。\n\n滚动升级是最常见的自动化工具。它们在速度和成本之间取得了很好的平衡，也减少手动工作和风险。\n升级生产集群时，你现有的所有工作负载仍将被部署；只要你测试了它们的兼容性，你的升级就应该是可自动化的。\n使用滚动升级时的进一步考虑包括：\n\n滚动升级可能会很慢，具体取决于你的规模。\n在升级期间，你可能需要协调控制器、守护进程或插件升级。\n你可能无法进行集群范围的更改，例如添加可用区或更改架构。\n\n04(金丝雀升级)Canary 应用程序部署一次为应用程序的新版本提供少量流量。Canary 升级可以被认为是具有蓝/绿优势的滚动升级。\n\n通过 Canary 升级，你将使用要部署的版本创建一个新的 Kubernetes 集群。然后添加一个小型数据平面并将你现有的应用程序以较小的规模部署到新集群。通过负载均衡器配置、DNS 循环或服务网格将新的集群工作负载添加到现有的生产流量中。\n现在，你可以监控流向新集群的流量，慢慢扩展新集群中的工作负载并缩减旧集群中的工作负载。你可以一次完成一项工作，并且可以根据自己的习惯缓慢或快速完成。如果任何单个工作负载开始出现错误，你可以缩减新集群中的单个工作负载，使其自动使用旧集群。\nCanary 集群升级的好处包括：\n\n新集群更容易创建和验证。\n你可以在升级期间跳过次要 Kubernetes 版本（例如，1.16 到 1.20）。\n可以在每个团队的基础上选择加入应用程序部署。\n由于增加的流量使用，错误的影响最小。\n你可以在升级期间进行大型基础架构更改。\n集群从小规模开始，因此基础设施成本较低，你可以在扩展时预热缓存和负载均衡器。\n\n如果你想进行较大的更改（例如更改架构）或者你想添加额外的可用区，那么 Canary 是一个不错的选择。通过启动较小的集群并根据工作负载增加它，你可以确保在新实例更高效或工作负载请求和限制发生变化时不会过度配置基础设施。\n与任何事情一样，需要权衡取舍。使用金丝雀部署时，你应该注意以下一些问题：\n\n回滚应用程序可能需要手动干预来更改负载均衡器或缩小新集群的规模。\n调试应用程序可能更难，因为你需要知道发生了哪些集群错误。\n如果你有数十个或数百个集群，随着集群的升级，你的集群数量可能会增加 50% 或更多。\nCanary 是最复杂的升级策略，但它受益于自动化部署、健康检查和性能监控。\n\n结论无论你选择哪种升级策略，重要的是要了解它们的工作原理以及随着 Kubernetes 使用量的增长可能出现的任何问题。你需要有一个升级策略，因为 Kubernetes 有频繁的发布和偶尔的错误。\n与新版本保持同步可能是你的基础设施安全流程的重要组成部分，并使应用程序能够快速利用新功能。如果你部署了 Kubernetes 并迁移了所有工作负载，而没有考虑如何升级，那么现在是开始计划的最佳时机。\n如果你没有运行自己的 Kubernetes 集群的业务需求，我强烈建议你使用可用的托管 Kubernetes 选项之一。选择托管控制平面和数据平面可以为你每年节省数天或数周的规划和升级时间。每个托管选项可能执行不同的升级，但它们都允许你专注于工作负载和业务价值，而不是控制平面高可用性或数据平面兼容性。\n参考：https://www.kubernetes.org.cn/9763.html\n","categories":["CloudNative"],"tags":["k8s"]},{"title":"k8s常用命令","url":"/2022/01/05/CloudNative/k8s%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/","content":"创建https证书kubectl -n=filecoin-dev create secret tls cert --key dev.wxlovan.com.key   --cert dev.wxlovan.com.pem\n\n使用本地文件创建configmapkubectl -n=lgd create configmap nginx.conf --from-file nginx.conf\n\nk8s扩展deploymentkubectl -n=lsac-test scale --replicas=10 deployment identity-test\n\n查看之前的镜像版本号kubectl -n=lsac-demo rollout history deployment accountmanagement-demokubectl -n=lsac-demo rollout undo deployment accountmanagement-demo --to-revision=5\n\nk8s创建docker secretkubectl create secret docker-registry secret --docker-server=registry.cn-hongkong.aliyuncs.com --docker-username=testname --docker-password=testpassword\n\nkubectl命令Find more information at: https://kubernetes.io/docs/reference/kubectl/overview/\nBasic Commands (Beginner):  create         Create a resource from a file or from stdin.  expose         使用 replication controller, service, deployment 或者 pod 并暴露它作为一个 新的Kubernetes Service  run            在集群中运行一个指定的镜像  set            为 objects 设置一个指定的特征 Basic Commands (Intermediate):  explain        查看资源的文档  get            显示一个或更多 resources  edit           在服务器上编辑一个资源  delete         Delete resources by filenames, stdin, resources and names, or by resources and label selector Deploy Commands:  rollout        Manage the rollout of a resource  scale          为 Deployment, ReplicaSet, Replication Controller 或者 Job 设置一个新的副本数量  autoscale      自动调整一个 Deployment, ReplicaSet, 或者 ReplicationController 的副本数量 Cluster Management Commands:  certificate    修改 certificate 资源.  cluster-info   显示集群信息  top            Display Resource (CPU/Memory/Storage) usage.  cordon         标记 node 为 unschedulable  uncordon       标记 node 为 schedulable  drain          Drain node in preparation for maintenance  taint          更新一个或者多个 node 上的 taints Troubleshooting and Debugging Commands:  describe       显示一个指定 resource 或者 group 的 resources 详情  logs           输出容器在 pod 中的日志  attach         Attach 到一个运行中的 container  exec           在一个 container 中执行一个命令  port-forward   Forward one or more local ports to a pod  proxy          运行一个 proxy 到 Kubernetes API server  cp             复制 files 和 directories 到 containers 和从容器中复制 files 和 directories.  auth           Inspect authorization Advanced Commands:  diff           Diff live version against would-be applied version  apply          通过文件名或标准输入流(stdin)对资源进行配置  patch          使用 strategic merge patch 更新一个资源的 field(s)  replace        通过 filename 或者 stdin替换一个资源  wait           Experimental: Wait for a specific condition on one or many resources.  convert        在不同的 API versions 转换配置文件  kustomize      Build a kustomization target from a directory or a remote url. Settings Commands:  label          更新在这个资源上的 labels  annotate       更新一个资源的注解  completion     Output shell completion code for the specified shell (bash or zsh) Other Commands:  api-resources  Print the supported API resources on the server  api-versions   Print the supported API versions on the server, in the form of &quot;group/version&quot;  config         修改 kubeconfig 文件  plugin         Provides utilities for interacting with plugins.  version        输出 client 和 server 的版本信息 Usage:  kubectl [flags] [options]","categories":["CloudNative"],"tags":["k8s"]},{"title":"nfs文件系统","url":"/2022/08/30/filesystem/nfs/","content":"NFS文件系统NFS概念文件系统就是通过软件对磁盘上的数据进行组织和管理的一种机制，对其的一种封装或透视。NFS，Network File System。顾名思义，网络文件系统，即通过网络，对在不同主机上的文件进行共享。\nNFS的原理\nUbuntu安装NFS(Centos是yum安装，nfs和nfs-utils)安装nfs-kernel-server\nsudo apt install nfs-kernel-server rpcbind -y\n配置开机启动\nsudo systemctl enable nfs-serversudo systemctl enable rpcbind\n创建对应目录\nsudo mkdir -p /data/sharesudo chmod 666 /data/share\n修改配置文件/etc/fstab\n/data/share 192.168.0.0/16(rw,no_root_squash,no_all_squash,async,no_subtree_check)\n重载数据\nsudo exportfs -rv\nshowmount查看情况\nsudo showmount -eExport list for fil-HYVE-ZEUS:/data/share 192.168.0.0/16\n安装客户端并挂载客户端上不需要启动nfs服务，只需要安装client即可\n#安装nfssudo apt install nfs-common -y#检测rpc服务rpcinfo -p# 查看服务器端挂在目录showmount -e 192.168.2.210\n挂载目录(也可以写入/etc/fstab)\nmount -t nfs 192.168.2.210:/data/share /data/share\n配置文件含义rw                # 客户端对共享的目录可读写ro                # 客户端对共享的目录只读不可写sync              # 同步模式，也就是把内存的数据实时写入硬盘，但这样会降低磁盘效率async             # 非同步模式，也就是每隔一段时间才会把内存的数据写入硬盘，能保证磁盘效率，但当异常宕机/断电时，会丢失内存里的数据no_root_squash    # 客户端挂载NFS共享目录后，客户端上的root用户不受这些挂载选项的限制，权限很大root_squash       # 跟no_root_squash相反，客户端上的root用户受到这些挂载选项的限制，被当成普通用户all_squash        # 客户端上的所有用户在使用NFS共享目录时都被限定为一个普通用户anonuid           # 上面的几个squash用于把客户端的用户限定为普通用户，而anouid用于限定这个普通用户的uid，这个uid与服务端的/etc/passwd文件相对应，如：anouid=1000                  # 比如我客户端用xiaoming这个用户去创建文件，那么服务端同步这个文件的时候，文件的属主会变成服务端的uid(1000)所对应的用户anongid           # 同上，用于限定这个普通用户的gid\n\n\n","categories":["文件系统"],"tags":["nfs"]},{"title":"glusterfs文件系统","url":"/2022/08/30/filesystem/glusterfs/","content":"glusterfs文件系统glusterfs概念\nGlusterFS系统是一个可扩展的网络文件系统，相比其他分布式文件系统，GlusterFS具有高扩展性、高可用性、高性能、可横向扩展等特点，并且其没有元数据服务器的设计，让整个服务没有单点故障的隐患。\n\nglusterfs分布式卷（单副本）\n\n分布式卷也成为哈希卷，多个文件以文件为单位在多个brick上，使用哈希算法随机存储。应用场景：大量文件优点：读/写性能好（只是相对于glusterfs其他类型的卷好些）缺点：如果存储或服务器故障，该brick上的数据将丢失如果在部署的时候不指定卷类型，默认是分布式卷brick数量没有限制\n\n创建分布式卷命令：\ngluster volume create data 192.168.7.12:/data/sda1\nglusterfs分布式卷（双副本或者更多副本）\n\n复制卷是将多个文件在多个brick上复制多份，brick的数目要与需要复制的份数相等，建议brick分布在不同的服务器上。应用场景：对可靠性高和读写性能要求高的场景优点：读性能好缺点：写性能差\n\n创建复制卷命令\ngluster volume create cache replica 2 192.168.4.200:/data/sdd/data 192.168.4.201:/data/sdd/data\n\nreplica：文件保存的份数\n\n命令#启动卷:gluster volume start volume_name#查看卷的信息：gluster volume info data#查看卷状态sudo gluster volume info data#卷停止gluster volume stop data#扩容卷gluster volume add-brick data 192.168.4.18:/data/sdb  force#磁盘平衡：(一般用不着)gluster volume rebalance data start#查看平衡状态：gluster volume rebalance data status#移除磁盘：gluster volume remove-brick data 192.168.4.21:/data/sda force#删除卷：gluster volume delete data#查看卷状态：gluster volume info#IO信息查看：gluster volume profile start | info | stop\n\n故障解决\n待定\n\n","categories":["文件系统"],"tags":["glusterfs"]},{"title":"Ubuntu18初始化","url":"/2021/12/22/linux/Ubuntu18%E5%88%9D%E5%A7%8B%E5%8C%96/","content":"\n配置apt仓库同步时间安装必要编辑下载工具\n\nsudo suadd_user_name=ubuntusudo mv /etc/apt/sources.list /etc/apt/sources.list.baksudo groupadd -g 1001 $add_user_namesudo useradd -d /home/$add_user_name -s /bin/bash -u 1001 -g 1001 -m $add_user_name#echo $add_user_name:IgvauW7igowEIRFb|chpasswdecho $add_user_name:123456|chpasswdecho &quot;$add_user_name ALL = (root) NOPASSWD:ALL&quot; | sudo tee /etc/sudoers.d/$add_user_namesudo chmod 0440 /etc/sudoers.d/$add_user_nameecho &#x27;deb http://mirrors.163.com/ubuntu/ bionic main restricted universe multiversedeb-src http://mirrors.163.com/ubuntu/ bionic main restricted universe multiversedeb http://mirrors.163.com/ubuntu/ bionic-security main restricted universe multiversedeb-src http://mirrors.163.com/ubuntu/ bionic-security main restricted universe multiversedeb http://mirrors.163.com/ubuntu/ bionic-updates main restricted universe multiversedeb-src http://mirrors.163.com/ubuntu/ bionic-updates main restricted universe multiversedeb http://mirrors.163.com/ubuntu/ bionic-proposed main restricted universe multiversedeb-src http://mirrors.163.com/ubuntu/ bionic-proposed main restricted universe multiversedeb http://mirrors.163.com/ubuntu/ bionic-backports main restricted universe multiversedeb-src http://mirrors.163.com/ubuntu/ bionic-backports main restricted universe multiverse &#x27; &gt; /etc/apt/sources.listsudo apt update -y &amp;&amp; sudo apt install ntpdate vim wget -yln -sf /usr/share/zoneinfo/Asia/Shanghai /etc/localtimentpdate ntp.aliyun.com","categories":["Linux"],"tags":["centos"]},{"title":"mdadm","url":"/2022/01/05/linux/mdadm/","content":"mdadm命令详解-C: 创建模式    -n #: 使用#个块设备来创建此RAID    -l #：指明要创建的RAID的级别    -a &#123;yes|no&#125;：自动创建目标RAID设备的设备文件    -c CHUNK_SIZE: 指明块大小,单位k    -x #: 指明空闲盘的个数-D：显示raid的详细信息    mdadm-D /dev/md#    管理模式：        -f: 标记指定磁盘为损坏        -a: 添加磁盘        -r: 移除磁盘    观察md的状态：cat /proc/mdstat-C,--create 新建一个阵列-r,--remove 删除列出的设备，设备不可处于活动状态-A,--assemble 激活阵列-l,--level== 设置阵列级别-D,--detail 打印一个或多个阵列设备信息-n,--raid-devices= 指定阵列成员信息-s，-scan 扫描配置文件或/proc/mdstat得到阵列缺失信息-x或--spare-devices= 指定阵列中备用盘的数量-f,--fail 将列出的设备标记为故障-c,--chunk= 指定阵列的块大小。默认512K-a,--add 添加设备到阵列-G,--grow改变阵列大小和形态-v，--verbose 显示详细信息-S,--stop 停止阵列，释放所有资源\n\n创建配置文件mdadm -Ds &gt;/etc/mdadm.conf\n\n创建raid1mdadm -C /dev/md1 -a yes -l1 -n 3 /dev/sdv /dev/sdw /dev/sdx\n\nmdadm创建raid0磁盘阵列mdadm -C /dev/md1 -a yes -l0 -n 5 /dev/sd&#123;b..f&#125;\n\nmdadm创建raid5磁盘阵列mdadm -C /dev/md51 -a yes -l 5 -n 5  /dev/sd&#123;b..f&#125;\n\n创建raid6mdadm -C /dev/md6 -a yes -l 6 -n 10 /dev/sd&#123;i..r&#125;1\n\n停止raid后扫描启动raidmdadm --assemble --scan -v\n\nraid5修复#标记为坏盘mdadm /dev/md51 -f /dev/sdq#删除损坏的磁盘mdadm /dev/md52 -r /dev/sdg#添加新磁盘mdadm /dev/md52 -a /dev/sdt#全部损坏后使用此命令mdadm --assemble --force /dev/md127 /dev/sdx /dev/sdv /dev/sdc /dev/sda /dev/sdy /dev/sdw\n\n清除磁盘raid信息mdadm --zero-superblock /dev/sdb\n\n","categories":["Linux"],"tags":["mdadm"]},{"title":"mongodb","url":"/2022/01/05/linux/mongodb/","content":"官网下载wget https://repo.mongodb.org/yum/redhat/7/mongodb-org/4.2/x86_64/RPMS/mongodb-org-server-4.2.5-1.el7.x86_64.rpmwget https://repo.mongodb.org/yum/redhat/7/mongodb-org/4.2/x86_64/RPMS/mongodb-org-mongos-4.2.5-1.el7.x86_64.rpmwget https://repo.mongodb.org/yum/redhat/7/mongodb-org/4.2/x86_64/RPMS/mongodb-org-shell-4.2.5-1.el7.x86_64.rpmwget https://repo.mongodb.org/yum/redhat/7/mongodb-org/4.2/x86_64/RPMS/mongodb-org-tools-4.2.5-1.el7.x86_64.rpmwget https://repo.mongodb.org/yum/redhat/7/mongodb-org/4.2/x86_64/RPMS/mongodb-org-4.2.5-1.el7.x86_64.rpm\n运行命令安装rpm -ivh mongodb-org-*mkdir -p /data/mongodb/logmkdir -p /data/mongodb/data\n更改配置文件  vim /etc/mongod.conf# mongod.conf# for documentation of all options, see:#   http://docs.mongodb.org/manual/reference/configuration-options/# where to write logging data.systemLog:  destination: file  logAppend: true  path: /data/mongodb/log/mongod.log# Where and how to store data.storage:  dbPath: /data/mongodb/lib  journal:    enabled: true#  engine:#  wiredTiger:# how the process runsprocessManagement:  fork: true  # fork and run in background  pidFilePath: /var/run/mongodb/mongod.pid  # location of pidfile  timeZoneInfo: /usr/share/zoneinfo# network interfacesnet:  port: 27017  bindIp: 0.0.0.0  # Enter 0.0.0.0,:: to bind to all IPv4 and IPv6 addresses or, alternatively, use the net.bindIpAll setting.security:  authorization: enabled\n\n创建相应文件夹并更改文件权限mkdir -p /data/mongodb/log/mkdir /data/mongodb/libmkdir -p /var/run/mongodb/chown -R mongod.mongod /data/mongodbchown -R mongod.mongod /var/run/mongodb/\n\n启动并设置开机启动mongodbsystemctl start mongodsystemctl enable mongodsystemctl status mongod\n\n连接本地mongo并建新库和设置密码，连接命令：mongouse admindb.createUser(&#123; user: &quot;admin&quot;, pwd: &quot;ByOHZKqKsCl1DwuM&quot;, roles: [&#123; role: &quot;userAdminAnyDatabase&quot;, db: &quot;admin&quot; &#125;] &#125;)#退出mongodb，再重新登陆mongouse admindb.auth(&#x27;admin&#x27;,&#x27;ByOHZKqKsCl1DwuM&#x27;)use lsacdb.createUser(&#123; user: &quot;lsac&quot;, pwd: &quot;BRDhPLwCV533UeXh&quot;, roles: [&#123; role: &quot;dbOwner&quot;, db: &quot;lsac&quot; &#125;] &#125;)\n退出并重新登陆mongodbuse lsacdb.auth(&#x27;lsac&#x27;,&#x27;BRDhPLwCV533UeXh&#x27;)#添加一个数据并使数据库出现db.lsac.insert(&#123;&quot;name&quot;:&quot;huihui&quot;&#125;)mongodb权限详解权限详解内建角色：数据库用户角色：read、readWrite；数据库管理角色：dbAdmin、dbOwner、userAdmin；集群管理角色：   clusterAdmin、clusterManager、clusterMonitor、hostManager；备份恢复角色：   backup、restore；所有数据库角色：readAnyDatabase、readWriteAnyDatabase、userAdminAnyDatabase、dbAdminAnyDatabase超级用户角色：   root；  这里还有几个角色间接或直接提供了系统超级用户的访问（dbOwner 、userAdmin、userAdminAnyDatabase）内部角色：          __system；------------------------------------------------------------------------------------------角色说明：Read：                             允许用户读取指定数据库readWrite：                     允许用户读写指定数据库dbAdmin：                      允许用户在指定数据库中执行管理函数，如索引创建、删除，查看统计或访问system.profileuserAdmin：                    允许用户向system.users集合写入，可以找指定数据库里创建、删除和管理用户dbOwner：                       允许在当前DB中执行任意操作readAnyDatabase：          赋予用户所有数据库的读权限，只在admin数据库中可用readWriteAnyDatabase： 赋予用户所有数据库的读写权限，只在admin数据库中可用userAdminAnyDatabase：赋予用户所有数据库管理User的权限，只在admin数据库中可用dbAdminAnyDatabase：   赋予管理所有数据库的权限，只在admin数据库中可用root：                                 超级账号，超级权限，只在admin数据库中可用。------------------------------------------------------------------------------------------集群管理角色：clusterAdmin：                  赋予管理集群的最高权限，只在admin数据库中可用clusterManager：               赋予管理和监控集群的权限clusterMonitor：                赋予监控集群的权限，对监控工具具有readonly的权限hostManager：                   赋予管理Server修改密码方法1：db.changeUserPassword(&quot;usertest&quot;,&quot;changepass&quot;);方法2：db.updateUser(&quot;usertest&quot;,&#123;pwd:&quot;changepass1&quot;&#125;)；修改权限修改权限db.updateUser(&quot;usertest&quot;,&#123;roles:[ &#123;role:&quot;read&quot;,db:&quot;testDB&quot;&#125; ]&#125;)db.grantRolesToUser(&quot;usertest&quot;, [&#123;role:&quot;readWrite&quot;, db:&quot;testDB&quot;&#125;,&#123;role:&quot;read&quot;, db:&quot;testDB&quot;&#125;])   # 修改权限db.revokeRolesFromUser(&quot;usertest&quot;,[&#123;role:&quot;read&quot;, db:&quot;testDB&quot;&#125;])   # 删除权限：删除用户删除用户db.dropUser(&#x27;usertest&#x27;)","categories":["Linux"],"tags":["mongodb"]},{"title":"Centos7初始化脚本","url":"/2021/12/21/linux/Centos7%E5%88%9D%E5%A7%8B%E5%8C%96/","content":"\n设置时区关闭selinux关闭防火墙更改yum仓库同步时间\n\ncat &lt;&lt; EOF &gt;&gt; init.shtimedatectl set-timezone Asia/Shanghaised -i &#x27;s/SELINUX=enforcing/SELINUX=disabled/g&#x27; /etc/selinux/configsystemctl stop firewalldsystemctl disable firewalldsetenforce 0rm -rf /etc/yum.repos.d/*curl -o /etc/yum.repos.d/CentOS-Base.repo http://mirrors.aliyun.com/repo/Centos-7.reposed -i -e &#x27;/mirrors.cloud.aliyuncs.com/d&#x27; -e &#x27;/mirrors.aliyuncs.com/d&#x27; /etc/yum.repos.d/CentOS-Base.repoyum install wget -ywget -O /etc/yum.repos.d/epel.repo http://mirrors.aliyun.com/repo/epel-7.reposed -i &#x27;s|^#baseurl=https://download.fedoraproject.org/pub|baseurl=https://mirrors.aliyun.com|&#x27; /etc/yum.repos.d/epel*sed -i &#x27;s|^metalink|#metalink|&#x27; /etc/yum.repos.d/epel*yum -y install ntp/usr/sbin/ntpdate ntp1.aliyun.comecho &quot;* 4 * * * /usr/sbin/ntpdate ntp1.aliyun.com &gt; /dev/null 2&gt;&amp;1&quot; &gt;&gt; /var/spool/cron/rootsystemctl restart crond.serviceyum -y install vimEOFsh init.sh","categories":["Linux"],"tags":["centos"]},{"title":"yum","url":"/2021/12/23/linux/yum/","content":"yum命令详解先来看子命令，再看 options。yum –help 查看帮助\n子命令还是挺多的，有 29 个。\ncheck检查rpmdb中的问题\ncheck-update检查可用的软件包更新\nclean清除缓存数据\nsudo yum cleanError: clean requires an option: headers, packages, metadata, dbcache, plugins, expire-cache, rpmdb, all$ sudo yum clean all\n\ndeplist列出某软件的依赖：\n$ yum deplist docker\n\ndistribution-synchronization将安装的软件包同步到最新的可用版本\n$ sudo yum distribution-synchronization\n\ndowngrade降级一个包\nerase从系统中删除一个或多个软件包，remove 是 erase 的一个别名。使用以下命令查看别名：\n$ yum help erase\nfs作用于主机的文件系统数据，主要用于删除主机中的文档/语言。\nfssnapshot创建文件系统快照，或列出/删除当前快照。\ngroups显示使用组\n$ sudo yum groups$ yum grouplist$ yum group info Haskell$ yum group list Haskell\n\nhelp显示帮助信息，要显示子命令的帮助信息，可以这样：\n$ yum help groups\n\nhistory查看使用 yum 的历史，非常好用：\n$ sudo yum history\n\ninfo查看软件包信息：\n$ yum info docker\n\ninstall安装一个包，很常用\nlist列出一个包，可以使用通配符：\n$ yum list dock*\n\nload-transaction从文件名加载已保存的软件包\nmakecache生成元数据缓存\n$ sudo yum makecache\n数据缓存到了 /var/lib/yum\nprovides$ yum provides docker\n查看包的提供者\nreinstall重新安装\nrepolist列出 repo：\n$ sudo yum repolist\nrepo-pkgs根据上一步列出的 repo id 进行操作：\n$ sudo yum repo-pkgs epel list\nsearch搜索包，跟 yum list 差不多的功能，不过这个 search 不用手动添加通配符，并且有高亮提示。\nshell运行一个交互式 shell，来执行 yum 的各种子命令：\n$ sudo yum shell\n\nlist dockerlist docker*update更新包，升级所有包同时也升级软件和系统内核\n\nupdate-minimal更新包，但是会匹配当前系统\nupdateinfo查看更新信息\nupgrade只升级所有包，不升级软件和系统内核\nversion显示物理机及各种repo的版本\n安装软件指定 repo$ sudo yum install nginx –enablerepo=epel只下载，不安装$ yum -y install google-chrome-stable –downloadonly –downloaddir=./\n","categories":["Linux"],"tags":["yum"]},{"title":"goStudy","url":"/2022/08/30/Go/goStudy/","content":"GoStudystart\n","categories":["Go"],"tags":["Go"]}]